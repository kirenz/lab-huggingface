{
  "hash": "d791d0856d65abdefc8b0604aa535e27",
  "result": {
    "markdown": "---\ntitle: Getting Started with Transformers\ntitle-slide-attributes:\n  data-background-image: ../images/logo.png\n  data-background-size: contain\n  data-background-opacity: \"0.5\"\nlang: en\nsubtitle: HuggingFace\nauthor: Jan Kirenz\nexecute:\n  eval: false\n  echo: true\nhighlight-style: github\nformat:\n  revealjs: \n    toc: true\n    toc-depth: 1\n    embed-resources: false\n    theme: [dark, ../custom.scss]  \n    incremental: true\n    transition: slide\n    background-transition: fade\n    transition-speed: slow\n    code-copy: true\n    code-line-numbers: true\n    smaller: false\n    scrollable: true\n    slide-number: c\n    preview-links: auto\n    chalkboard: \n      buttons: false\n   # logo: ../images/logo.png\n    footer: Jan Kirenz\n---\n\n# Transformers intro\n\n## Learning goals\n\n\n1. Transformer neural networks can be used to tackle a wide range of tasks in natural language processing and beyond.\n2. Transfer learning allows one to adapt Transformers to specific tasks.\n3. The `pipeline()` function from the `transformers` library can be used to run inference with models from the [Hugging Face Hub](https://huggingface.co/models).\n\n\n\n## Why Transformers?\n\n- Deep learning is currently undergoing a period of rapid progress across a wide variety of domains, including: \n\n* üìñ Natural language processing\n* üëÄ Computer vision\n* üîä Audio\n* and many more!\n\n- The main driver of these breakthroughs is the **Transformer** -- a novel **neural network** developed by [Google researchers in 2017](https://arxiv.org/abs/1706.03762). \n\n## Transformers examples\n\n* üíª They can **generate code** as in products like [GitHub Copilot](https://copilot.github.com/), which is based on OpenAI's family of [GPT models](https://huggingface.co/gpt2?text=My+name+is+Clara+and+I+am).\n\n* ‚ùì They can be used for **improve search engines**, like [Google did](https://www.blog.google/products/search/search-language-understanding-bert/) with a Transformer called [BERT](https://huggingface.co/bert-base-uncased).\n\n* üó£Ô∏è They can **process speech in multiple languages** to perform speech recognition, speech translation, and language identification. For example, Facebook's [XLS-R model](https://huggingface.co/spaces/facebook/XLS-R-2B-22-16) can automatically transcribe audio in one language to another!\n\n## Transfer learning \n\n- Training Transformer models **from scratch** involves **a lot of resources** (compute, data, and days to train)\n\n- With **transfer learning**, it is possible to adapt a model that has been trained from scratch (usually called a **pretrained model**) for a new, but similar task.\n\n## Fine tuning\n\n- Fine-tuning can be used as a special case of transfer learning where you use new data to continue training the model on the new task.\n\n- The models that we'll be looking at in this tutorial are all examples of fine-tuned models\n\n\n## Transfer learning \n\nYou can learn more about the transfer learning process in the video below:\n\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube-nocookie.com/embed/BqqfQnyjmgg?si=P09F30TBQvBttyXC\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n\n# Hugging Face Transformers\n\nThe [Hugging Face Transformers library](https://github.com/huggingface/transformers) provides a unified API across dozens of Transformer architectures, as well as the means to train models and run inference with them. \n\n<!---\n \n- So to get started, let's install the library with the following command:\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n!pip install transformers[sentencepiece]\n```\n:::\n\n\nNow that we've installed the library, let's take a look at some applications! \n--->\n\n## Pipelines for Transformers\n\n- The fastest way to learn what Transformers can do is via the `pipeline()` function. \n\n- This function loads a model from the Hugging Face Hub and takes care of all the preprocessing and postprocessing steps that are needed to convert inputs into predictions:\n\n. . .\n\n<img src=\"https://github.com/huggingface/workshops/blob/main/nlp-zurich/images/pipeline.png?raw=1\" alt=\"Alt text that describes the graphic\" title=\"Title text\" width=800>\n\n## What happens inside the pipeline function? \n\n\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube-nocookie.com/embed/1pedAIvTWXk?si=WhdZ1fe6iQokgH2X\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n\n\n# Setup\n\nImport the pipeline:\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nfrom transformers import pipeline\n```\n:::\n\n\n## Text input\n\n\n- We need a snippet of text for our models to analyze, so let's use the following (fictious!) customer feedback about a certain online order:\n\n. . .\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ntext = \"\"\"Dear Amazon, last week I ordered an Optimus Prime action figure \\\nfrom your online store in Germany. Unfortunately, when I opened the package, \\\nI discovered to my horror that I had been sent an action figure of Megatron \\\ninstead! As a lifelong enemy of the Decepticons, I hope you can understand my \\\ndilemma. To resolve the issue, I demand an exchange of Megatron for the \\\nOptimus Prime figure I ordered. Enclosed are copies of my records concerning \\\nthis purchase. I expect to hear from you soon. Sincerely, Bumblebee.\"\"\"\n```\n:::\n\n\n## Create text wrapper\n\n- Let's create a simple wrapper so that we can pretty print out texts:\n\n. . .\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nimport textwrap\n\nwrapper = textwrap.TextWrapper(\n            width=80, \n            break_long_words=False, \n            break_on_hyphens=False\n          )\n```\n:::\n\n\n# Text classification\n\nLet's start with one of the most common tasks in NLP: text classification\n\n## Analyze sentiment\n\n- Now suppose that we'd like to predict the _sentiment_ of this text, i.e. whether the feedback is positive or negative. \n\n- This is a special type of text classification that is often used in industry to aggregate customer feedback across products or services. \n\n\n## Tokens\n\n- The example below shows how a Transformer like BERT converts the inputs into atomic chunks called **tokens** which are then fed through the network to produce a single prediction:\n\n<img src=\"https://github.com/huggingface/workshops/blob/main/nlp-zurich/images/clf_arch.png?raw=1\" alt=\"Alt text that describes the graphic\" title=\"Title text\" width=600>\n\n## Pipeline  {.smaller}\n\n- We need to specify the task in the `pipeline()` function as follows;\n\n. . .\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nsentiment_pipeline = pipeline('text-classification')\n```\n:::\n\n\n- When you run this code, you'll see a message about which Hub model is being used by default. \n\n- In this case, the `pipeline()` function loads the `distilbert-base-uncased-finetuned-sst-2-english` model, which is a small BERT variant trained on [SST-2](https://paperswithcode.com/sota/sentiment-analysis-on-sst-2-binary) which is a sentiment analysis dataset.\n\n:::{.callout-note}\nüí° The first time you execute the code, the model will be automatically downloaded from the Hub and cached for later use! \n:::\n\n## Run pipeline {.smaller}\n\n- Now we are ready to run our example through pipeline and look at some predictions:\n\n. . .\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nsentiment_pipeline(text)\n```\n:::\n\n\n- `Output`: [{'label': 'NEGATIVE', 'score': 0.9015464186668396}]\n\n- The model predicts negative sentiment with a high confidence which makes sense given that we have a disgruntled customer. \n\n- You can also see that the pipeline returns a list of Python dictionaries with the predictions. \n\n- We can also pass several texts at the same time in which case we would get several dicts in the list for each text one.\n\n# Named entity recognition (NER)\n\n## Basics\n\n- Instead of just finding the overall sentiment, let's see if we can extract **entities** such as organizations, locations, or individuals from the text. \n\n- This task is called named entity recognition, or NER for short. \n\n## Predict class for echa token\n\n- Instead of predicting just a class for the whole text **a class is predicted for each token**, as shown in the example below:\n\n. . .\n\n<img src=\"https://github.com/huggingface/workshops/blob/main/nlp-zurich/images/ner_arch.png?raw=1\" alt=\"Alt text that describes the graphic\" title=\"Title text\" width=600>\n\n## Pipeline\n\n- We just load a pipeline for NER without specifying a model. \n\n- This will load a default BERT model that has been trained on the [CoNLL-2003](https://huggingface.co/datasets/conll2003) dataset:\n\n. . .\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nner_pipeline = pipeline('ner')\n```\n:::\n\n\n## Merge entities {.smaller}\n\n- When we pass our text through the model, we now get a long list of Python dictionaries, where each dictionary corresponds to one detected entity. \n\n- Since multiple tokens can correspond to a a single entity, we can apply an aggregation strategy that merges entities if the same class appears in consequtive tokens:\n\n. . .\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nentities = ner_pipeline(text, aggregation_strategy=\"simple\")\nprint(entities)\n```\n:::\n\n\n- `Output`: [{'entity_group': 'ORG', 'score': 0.87900954, 'word': 'Amazon', 'start': 5, 'end': 11}, {'entity_group': 'MISC', 'score': 0.9908588, 'word': 'Optimus Prime', 'start': 36, 'end': 49}, {'entity_group': 'LOC', 'score': 0.9997547, 'word': 'Germany', 'start': 90, 'end': 97}, {'entity_group': 'MISC', 'score': 0.55656713, 'word': 'Mega', 'start': 208, 'end': 212}, {'entity_group': 'PER', 'score': 0.5902563, 'word': '##tron', 'start': 212, 'end': 216}, {'entity_group': 'ORG', 'score': 0.6696913, 'word': 'Decept', 'start': 253, 'end': 259}, {'entity_group': 'MISC', 'score': 0.4983487, 'word': '##icons', 'start': 259, 'end': 264}, {'entity_group': 'MISC', 'score': 0.77536064, 'word': 'Megatron', 'start': 350, 'end': 358}, {'entity_group': 'MISC', 'score': 0.987854, 'word': 'Optimus Prime', 'start': 367, 'end': 380}, {'entity_group': 'PER', 'score': 0.81209683, 'word': 'Bumblebee', 'start': 502, 'end': 511}]\n\n## Clean the output\n\n- This isn't very easy to read, so let's clean up the outputs a bit:\n\n. . .\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nfor entity in entities:\n    print(f\"{entity['word']}: {entity['entity_group']} ({entity['score']:.2f})\")\n```\n:::\n\n\n```json\nAmazon: ORG (0.88)  \nOptimus Prime: MISC (0.99)  \nGermany: LOC (1.00)  \nMega: MISC (0.56)  \n##tron: PER (0.59)  \nDecept: ORG (0.67)  \n##icons: MISC (0.50)  \nMegatron: MISC (0.78)  \nOptimus Prime: MISC (0.99)  \nBumblebee: PER (0.81)  \n```\n\n## Findings\n\n- It seems that the model found most of the named entities but was confused about \"Megatron\" andn \"Decepticons\", which are characters in the transformers franchise. \n\n- This is no surprise since the original dataset probably did not contain many transformer characters. For this reason it makes sense to further fine-tune a model on your on dataset!\n\n\n# Question answering\n\n## Basics {.smaller}\n\n- In this task, the model is given a **question** and a **context** and needs to find the answer to the question within the context. \n\n- This problem can be rephrased as a classification problem: For each token the model needs to predict whether it is the start or the end of the answer. \n\n## Basics\n\n- In the end we can extract the answer by looking at the span between the token with the highest start probability and highest end probability:\n\n. . .\n\n<img src=\"https://github.com/huggingface/workshops/blob/main/nlp-zurich/images/qa_arch.png?raw=1\" alt=\"Alt text that describes the graphic\" title=\"Title text\" width=600>\n\n\n## Pipeline\n\n- we load the model by specifying the task in the `pipeline()` function:\n\n. . .\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nqa_pipeline = pipeline(\"question-answering\")\n```\n:::\n\n\n- This default model is trained on the famous [SQuAD dataset](https://huggingface.co/datasets/squad).\n\n## Ask question\n\n- Let's see if we can ask it what the customer wants:\n\n. . .\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nquestion = \"What does the customer want?\"\n\noutputs = qa_pipeline(question=question, context=text)\noutputs\n```\n:::\n\n\n. . .\n\n```json\n{'score': 0.6312916874885559,\n 'start': 335,\n 'end': 358,\n 'answer': 'an exchange of Megatron'}\n```\n\n# Text summarization\n\n## Basics {background-color=\"white\"}\n\n- Generation is much more computationally demanding since we usually generate one token at a time and need to run this several times. \n\n- An example for how this process works is shown below:\n\n. . .\n\n<img src=\"https://github.com/huggingface/workshops/blob/main/nlp-zurich/images/gen_steps.png?raw=1\" alt=\"Alt text that describes the graphic\" title=\"Title text\" width=600>\n\n## Pipeline\n\n- A popular task involving generation is summarization\n\n. . .\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\nsummarization_pipeline = pipeline(\"summarization\")\n```\n:::\n\n\n- This model was trained on the [CNN/Dailymail dataset](https://huggingface.co/datasets/cnn_dailymail) to summarize news articles.\n\n## Output\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\noutputs = summarization_pipeline(text, max_length=80, clean_up_tokenization_spaces=True)\n\nprint(wrapper.fill(outputs[0]['summary_text']))\n```\n:::\n\n\n- `Output`:  Bumblebee ordered an Optimus Prime action figure from your online store in Germany. Unfortunately, when I opened the package, I discovered to my horror\nthat I had been sent an action figure of Megatron instead. As a lifelong enemy\nof the Decepticons, I hope you can understand my dilemma.\n\n# Translation\n\n## Basics\n\n- But what if there is no model in the language of my data? \n\n- You can still try to translate the text. \n\n- The [Helsinki NLP team](https://huggingface.co/models?pipeline_tag=translation&sort=downloads&search=Helsinkie-NLP) has provided over 1,000 language pair models for translation. \n\n## Pipeline\n\n- Translate English to German:\n\n. . .\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\ntranslator = pipeline(\"translation_en_to_de\", model=\"Helsinki-NLP/opus-mt-en-de\")\n```\n:::\n\n\n## Output {.smaller}\n\n- Let's translate our text to German:\n\n. . .\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\noutputs = translator(text, clean_up_tokenization_spaces=True, min_length=100)\n\nprint(wrapper.fill(outputs[0]['translation_text']))\n```\n:::\n\n\n- `Output`: Sehr geehrter Amazon, letzte Woche habe ich eine Optimus Prime Action Figur aus Ihrem Online-Shop in Deutschland bestellt. Leider, als ich das Paket √∂ffnete, entdeckte ich zu meinem Entsetzen, dass ich stattdessen eine Action Figur von Megatron geschickt worden war! Als lebenslanger Feind der Decepticons, Ich hoffe, Sie k√∂nnen mein Dilemma verstehen. Um das Problem zu l√∂sen, Ich fordere einen Austausch von Megatron f√ºr die Optimus Prime Figur habe ich bestellt. Eingeschlossen sind Kopien meiner Aufzeichnungen √ºber diesen Kauf. Ich erwarte,\nvon Ihnen bald zu h√∂ren. Aufrichtig, Bumblebee.\n\n## Findings\n\n- We can see that the text is clearly not perfectly translated, but the core meaning stays the same. \n\n- Another  application of translation models is data augmentation via backtranslation\n\n# Zero-shot classification\n\n\n## Basics\n\n- In zero-shot classification the model receives a text and a list of candidate labels and determines which labels are compatible with the text. \n\n- Instead of having fixed classes this allows for flexible classification without any labelled data! \n\n- Usually this is a good first baseline!\n\n\n## Pipeline\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\nzero_shot_classifier = pipeline(\"zero-shot-classification\",\n                                model=\"vicgalle/xlm-roberta-large-xnli-anli\")\n```\n:::\n\n\n## Text input\n\nLet's have a look at an example:\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\ntext = 'Dieses Tutorial ist gro√üartig! Ich hoffe, dass jemand von Hugging Face meine Hochschule besuchen wird :)'\n\nclasses = ['Treffen', 'Arbeit', 'Digital', 'Reisen']\n```\n:::\n\n\n## Pipeline\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\nzero_shot_classifier(text, classes, multi_label=True)\n```\n:::\n\n\n```json\n{'sequence': 'Dieses Tutorial ist gro√üartig! Ich hoffe, dass jemand von Hugging Face meine Hochschule besuchen wird :)',\n 'labels': ['Digital', 'Arbeit', 'Treffen', 'Reisen'],\n 'scores': [0.7426563501358032,\n  0.6590237021446228,\n  0.517701268196106,\n  0.011237525381147861]}\n```\n\n- For longer and more domain specific examples this approach might suffer.\n\n# Going beyond text\n\nTransformers can also be used for domains other than NLP! \n\n## Basics {.smaller}\n\nThere are many more pipelines that you can experiment with\n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\nfrom transformers import pipelines\nfor task in pipelines.SUPPORTED_TASKS:\n    print(task)\n```\n:::\n\n\n```markdown\naudio-classification\nautomatic-speech-recognition\nfeature-extraction\ntext-classification\ntoken-classification\nquestion-answering\ntable-question-answering\nvisual-question-answering\ndocument-question-answering\nfill-mask\nsummarization\ntranslation\ntext2text-generation\ntext-generation\nzero-shot-classification\nzero-shot-image-classification\nzero-shot-audio-classification\nconversational\nimage-classification\nimage-segmentation\nimage-to-text\nobject-detection\nzero-shot-object-detection\ndepth-estimation\nvideo-classification\nmask-generation\n````\n\n## Computer vision\n\n- Transformer models have also entered computer vision. Check out the DETR model on the [Hub](https://huggingface.co/facebook/detr-resnet-101-dc5):\n\n. . .\n\n<img src=\"https://github.com/huggingface/workshops/blob/main/nlp-zurich/images/object_detection.png?raw=1\" alt=\"Alt text that describes the graphic\" title=\"Title text\" width=400>\n\n## Audio\n\n- Another promising area is audio processing (especially Speech2Text) \n\n- See for example the [wav2vec2 model](https://huggingface.co/facebook/wav2vec2-base-960h):\n\n. . .\n\n<img src=\"https://github.com/huggingface/workshops/blob/main/nlp-zurich/images/speech2text.png?raw=1\" alt=\"Alt text that describes the graphic\" title=\"Title text\" width=400>\n\n## Table QA\n\n- Finally, a lot of real world data is still in form of tables. \n\n- Being able to query tables is very useful and with [TAPAS](https://huggingface.co/google/tapas-large-finetuned-wtq) you can do tabular question-answering:\n\n. . .\n\n<img src=\"https://github.com/huggingface/workshops/blob/main/nlp-zurich/images/tapas.png?raw=1\" alt=\"Alt text that describes the graphic\" title=\"Title text\" width=400>\n\n# Resources\n\nüëâ [click here to access hands-on Transformers exercices](https://github.com/NielsRogge/Transformers-Tutorials)\n\n\n\n\n# What's next? {background-image=\"../images/logo.png\" background-opacity=\"0.5\"}\n\n**Congratulations! You have completed this tutorial** üëç\n\n**Next, you may want to go back to the [lab's website](https://kirenz.github.io/lab-huggingface/)**\n\n*The slides are mainly based on a toolkit provided by Hugging Face's Lewis Tunstall and the book [_Natural Language Processing with Transformers_](https://transformersbook.com/).*\n\n",
    "supporting": [
      "transformers_files"
    ],
    "filters": [],
    "includes": {}
  }
}