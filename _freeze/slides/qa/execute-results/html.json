{
  "hash": "c264eaaeeb2c4ee97b989a74dcbf4312",
  "result": {
    "markdown": "---\ntitle: Question Answering\ntitle-slide-attributes:\n  data-background-image: ../images/logo.png\n  data-background-size: contain\n  data-background-opacity: \"0.5\"\nlang: en\nsubtitle: with Hugging Face Pipelines\nauthor: Jan Kirenz\nexecute:\n  eval: false\n  echo: true\nhighlight-style: github\nformat:\n  revealjs: \n    toc: true\n    toc-depth: 1\n    embed-resources: false\n    theme: [dark, ../custom.scss]  \n    incremental: true\n    transition: slide\n    background-transition: fade\n    transition-speed: slow\n    code-copy: true\n    code-line-numbers: true\n    smaller: false\n    scrollable: true\n    slide-number: c\n    preview-links: auto\n    chalkboard: \n      buttons: false\n   # logo: ../images/logo.png\n    footer: Jan Kirenz\n---\n\n# Question Answering Hugging Face Pipeline\n\n# Setup\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nfrom transformers import pipeline\n```\n:::\n\n\n# Intuition\n\n![](../images/qa.png)\n\n- Question answering tasks return an answer given a question. \n\n\n## Use Cases\n\n\n- Automate the response to Frequently Asked Questions\n\n- Chat Bots\n\n\n## Common types \n\n- *Extractive* QA: The model extracts the answer from the original context.\n  - The context could be a provided text, a table or even HTML! This is usually solved with BERT-like models.\n\n- *Open Generative* QA: The model generates free text directly based on the context. \n\n- *Closed Generative QA*: In this case, no context is provided. The answer is completely generated by a model.\n\n\n## Closed-domain vs open-domain\n\n- Closed-domain models are restricted to a specific domain (e.g. legal, medical documents).\n\n- Open-domain models are not restricted to a specific domain.\n\n\n# Simple Pipeline Model\n\n## Define pipeline\n\n- Define the Pipeline^[Will be initialized with the default model [distilbert-base-cased-distilled-squad](https://huggingface.co/docs/transformers/model_doc/distilbert)]\n\n. . .\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nqa_model = pipeline(\"question-answering\")\n```\n:::\n\n\n## Provide question and context\n\n. . .\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nquestion = \"Where do I live?\"\n\ncontext = \"My name is Jan and I live in Stuttgart.\"\n```\n:::\n\n\n## Return the answer\n. . .\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nqa_model(question=question, context=context)\n```\n:::\n\n\n. . .\n\n\n```{bash}\n{'score': 0.9671180844306946, 'start': 29, 'end': 38, 'answer': 'Stuttgart'}\n\n```\n\n\n\n\n# What's next? {background-image=\"../images/logo.png\" background-opacity=\"0.5\"}\n\n**Congratulations! You have completed this tutorial** üëç\n\n**Next, you may want to go back to the [lab's website](https://kirenz.github.io/lab-huggingface/)**\n\n",
    "supporting": [
      "qa_files"
    ],
    "filters": [],
    "includes": {}
  }
}