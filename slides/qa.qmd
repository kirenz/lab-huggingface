---
title: Question Answering
title-slide-attributes:
  data-background-image: ../images/logo.png
  data-background-size: contain
  data-background-opacity: "0.5"
lang: en
subtitle: with Hugging Face Pipelines
author: Jan Kirenz
execute:
  eval: false
  echo: true
highlight-style: github
format:
  revealjs: 
    toc: true
    toc-depth: 1
    embed-resources: false
    theme: [dark, ../custom.scss]  
    incremental: true
    transition: slide
    background-transition: fade
    transition-speed: slow
    code-copy: true
    code-line-numbers: true
    smaller: false
    scrollable: true
    slide-number: c
    preview-links: auto
    chalkboard: 
      buttons: false
   # logo: ../images/logo.png
    footer: Jan Kirenz
---


# Question Answering Hugging Face Pipeline

# Setup

```{python}
from transformers import pipeline
```



# Intuition

![](../images/qa.png)

- Question answering tasks return an answer given a question. 


## Use Cases


- Automate the response to Frequently Asked Questions

- Chat Bots


## Common types 

- *Extractive* QA: The model extracts the answer from the original context.
  - The context could be a provided text, a table or even HTML! This is usually solved with BERT-like models.

- *Open Generative* QA: The model generates free text directly based on the context. 

- *Closed Generative QA*: In this case, no context is provided. The answer is completely generated by a model.


## Closed-domain vs open-domain

- Closed-domain models are restricted to a specific domain (e.g. legal, medical documents).

- Open-domain models are not restricted to a specific domain.


# Simple Pipeline Model

## Define pipeline

- Define the Pipeline^[Will be initialized with the default model [distilbert-base-cased-distilled-squad](https://huggingface.co/docs/transformers/model_doc/distilbert)]

. . .

```{python}
qa_model = pipeline("question-answering")

```

## Provide question and context

. . .

```{python}

question = "Where do I live?"

context = "My name is Jan and I live in Stuttgart."

```

## Return the answer
. . .

```{python}

qa_model(question=question, context=context)

```

. . .

```{bash}
{'score': 0.9671180844306946, 'start': 29, 'end': 38, 'answer': 'Stuttgart'}

```



# What's next? {background-image="../images/logo.png" background-opacity="0.5"}

**Congratulations! You have completed this tutorial** üëç

**Next, you may want to go back to the [lab's website](https://kirenz.github.io/lab-huggingface/)**
