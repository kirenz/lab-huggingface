---
title: Transformers intuition
title-slide-attributes:
  data-background-image: ../images/logo.png
  data-background-size: contain
  data-background-opacity: "0.5"
lang: en
subtitle: Basics
author: Jan Kirenz
execute:
  eval: false
  echo: true
highlight-style: github
format:
  revealjs: 
    toc: true
    toc-depth: 1
    embed-resources: false
    theme: [dark, ../custom.scss]  
    incremental: true
    transition: slide
    background-transition: fade
    transition-speed: slow
    code-copy: true
    code-line-numbers: true
    smaller: false
    scrollable: true
    slide-number: c
    preview-links: auto
    chalkboard: 
      buttons: false
   # logo: ../images/logo.png
    footer: Jan Kirenz
---


# Intuition

Transformers, explained: Understand the model behind GPT, BERT, and T5 by Dale Markowitz


##

{{< video https://www.youtube.com/embed/SZorAJ4I-sA width="1920" height="1080" >}}


## Important Architectures

- Convolutional Neural Network (CNN): Vision

- Recurrent Neural Network (RNN): Text

- Transformers: Text and more

## Main Characteristics of Transformers

- Positional Encoding

- Attention

- Self-Attention

## Popular Transformer Models

- GPT (Generative Pretrained Transformers): GPT-3, GPT-3.5, GPT-4 

- BERT (Bidirectional Encoder Representations from Transformers)

- T5 (Text-to-Text Transfer Transformer)

- RoBERTa (Robustly Optimized BERT Pretraining Approach)


# Hugging Face: Transforming the AI Landscape

![](https://huggingface.co/front/assets/huggingface_logo-noborder.svg)

## Overview

- AI research organization
- Develop cutting-edge machine learning models and tools
- Popular open-source Transformers models 
- Providing state-of-the-art pre-trained models and tools for a wide range of tasks

## Key Features

- Supports popular architectures like BERT, GPT, RoBERTa, and T5
- Easy-to-use API for fine-tuning and deploying models
  - *Fine-tuning* is the process of taking a pre-trained large language model (e.g. roBERTa) and then tweaking it with additional training data to make it perform a second similar task (e.g. sentiment analysis)
- Available in Python, with support for TensorFlow and PyTorch

## NLP Tasks

- Language translation
- Text generation
- Question answering
- Text summarization
- Sentiment analysis
- And more!


## Model Hub

The [Model Hub](https://huggingface.co/models) is a platform for sharing and discovering pre-trained models, contributed by the AI community.

- Access to thousands of pre-trained models
- Easy integration with the Transformers library
- Collaborative environment for researchers and developers

## Spaces

Discover ML apps made by the community: [Spaces](https://huggingface.co/spaces?sort=likes)

## Datasets

- Hugging Face also provides [Datasets](https://huggingface.co/datasets)
- Over 29,668 datasets available
- Efficient data loading and processing
- Easy integration with the Transformers library

## Pipelines 

![](https://huggingface.co/front/assets/huggingface_logo-noborder.svg)

- *Hugging Face* `Pipelines` cover common machine learning tasks

- Pre-built, easy-to-use abstractions (almost no code necessary)

- Simplify workflow


# State of the Art Examples

## 

{{< video https://www.youtube.com/embed/_rGXIXyNqpk width="1920" height="1080" >}}


## AgentGPT

Assemble, configure, and deploy autonomous AI Agents in your browser.


![AgentGPT](https://agentgpt.reworkd.ai/)


<https://github.com/reworkd/AgentGPT>

## Microsoft JARVIS

![](https://raw.githubusercontent.com/microsoft/JARVIS/main/assets/overview.jpg)


<https://github.com/microsoft/JARVIS>

# What's next? {background-image="../images/logo.png" background-opacity="0.5"}

**Congratulations! You have completed this tutorial** üëç

**Next, you may want to go back to the [lab's website](https://kirenz.github.io/lab-huggingface/)**