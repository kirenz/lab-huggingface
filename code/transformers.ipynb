{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Transformers intro\n",
                "\n",
                "## Learning goals\n",
                "\n",
                "\n",
                "1. Transformer neural networks can be used to tackle a wide range of tasks in natural language processing and beyond.\n",
                "2. Transfer learning allows one to adapt Transformers to specific tasks.\n",
                "3. The `pipeline()` function from the `transformers` library can be used to run inference with models from the [Hugging Face Hub](https://huggingface.co/models).\n",
                "\n",
                "\n",
                "\n",
                "## Why Transformers?\n",
                "\n",
                "- Deep learning is currently undergoing a period of rapid progress across a wide variety of domains, including: \n",
                "\n",
                "* üìñ Natural language processing\n",
                "* üëÄ Computer vision\n",
                "* üîä Audio\n",
                "* and many more!\n",
                "\n",
                "- The main driver of these breakthroughs is the **Transformer** -- a novel **neural network** developed by [Google researchers in 2017](https://arxiv.org/abs/1706.03762). \n",
                "\n",
                "## Transformers examples\n",
                "\n",
                "* üíª They can **generate code** as in products like [GitHub Copilot](https://copilot.github.com/), which is based on OpenAI's family of [GPT models](https://huggingface.co/gpt2?text=My+name+is+Clara+and+I+am).\n",
                "\n",
                "* ‚ùì They can be used for **improve search engines**, like [Google did](https://www.blog.google/products/search/search-language-understanding-bert/) with a Transformer called [BERT](https://huggingface.co/bert-base-uncased).\n",
                "\n",
                "* üó£Ô∏è They can **process speech in multiple languages** to perform speech recognition, speech translation, and language identification. For example, Facebook's [XLS-R model](https://huggingface.co/spaces/facebook/XLS-R-2B-22-16) can automatically transcribe audio in one language to another!\n",
                "\n",
                "## Transfer learning \n",
                "\n",
                "- Training Transformer models **from scratch** involves **a lot of resources** (compute, data, and days to train)\n",
                "\n",
                "- With **transfer learning**, it is possible to adapt a model that has been trained from scratch (usually called a **pretrained model**) for a new, but similar task.\n",
                "\n",
                "## Fine tuning\n",
                "\n",
                "- Fine-tuning can be used as a special case of transfer learning where you use new data to continue training the model on the new task.\n",
                "\n",
                "- The models that we'll be looking at in this tutorial are all examples of fine-tuned models\n",
                "\n",
                "\n",
                "## Transfer learning \n",
                "\n",
                "You can learn more about the transfer learning process in the video below:\n",
                "\n",
                "\n",
                "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube-nocookie.com/embed/BqqfQnyjmgg?si=P09F30TBQvBttyXC\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n",
                "\n",
                "# Hugging Face Transformers\n",
                "\n",
                "The [Hugging Face Transformers library](https://github.com/huggingface/transformers) provides a unified API across dozens of Transformer architectures, as well as the means to train models and run inference with them. \n",
                "\n",
                "<!---\n",
                " \n",
                "- So to get started, let's install the library with the following command:"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "!pip install transformers[sentencepiece]"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now that we've installed the library, let's take a look at some applications! \n",
                "--->\n",
                "\n",
                "## Pipelines for Transformers\n",
                "\n",
                "- The fastest way to learn what Transformers can do is via the `pipeline()` function. \n",
                "\n",
                "- This function loads a model from the Hugging Face Hub and takes care of all the preprocessing and postprocessing steps that are needed to convert inputs into predictions:\n",
                "\n",
                "\n",
                "\n",
                "<img src=\"https://github.com/huggingface/workshops/blob/main/nlp-zurich/images/pipeline.png?raw=1\" alt=\"Alt text that describes the graphic\" title=\"Title text\" width=800>\n",
                "\n",
                "## What happens inside the pipeline function? \n",
                "\n",
                "\n",
                "\n",
                "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube-nocookie.com/embed/1pedAIvTWXk?si=WhdZ1fe6iQokgH2X\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n",
                "\n",
                "\n",
                "# Setup\n",
                "\n",
                "Import the pipeline:"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "from transformers import pipeline"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Text input\n",
                "\n",
                "\n",
                "- We need a snippet of text for our models to analyze, so let's use the following (fictious!) customer feedback about a certain online order:\n",
                "\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "text = \"\"\"Dear Amazon, last week I ordered an Optimus Prime action figure \\\n",
                "from your online store in Germany. Unfortunately, when I opened the package, \\\n",
                "I discovered to my horror that I had been sent an action figure of Megatron \\\n",
                "instead! As a lifelong enemy of the Decepticons, I hope you can understand my \\\n",
                "dilemma. To resolve the issue, I demand an exchange of Megatron for the \\\n",
                "Optimus Prime figure I ordered. Enclosed are copies of my records concerning \\\n",
                "this purchase. I expect to hear from you soon. Sincerely, Bumblebee.\"\"\""
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Create text wrapper\n",
                "\n",
                "- Let's create a simple wrapper so that we can pretty print out texts:\n",
                "\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "import textwrap\n",
                "\n",
                "wrapper = textwrap.TextWrapper(\n",
                "            width=80, \n",
                "            break_long_words=False, \n",
                "            break_on_hyphens=False\n",
                "          )"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Text classification\n",
                "\n",
                "Let's start with one of the most common tasks in NLP: text classification\n",
                "\n",
                "## Analyze sentiment\n",
                "\n",
                "- Now suppose that we'd like to predict the _sentiment_ of this text, i.e. whether the feedback is positive or negative. \n",
                "\n",
                "- This is a special type of text classification that is often used in industry to aggregate customer feedback across products or services. \n",
                "\n",
                "\n",
                "## Tokens\n",
                "\n",
                "- The example below shows how a Transformer like BERT converts the inputs into atomic chunks called **tokens** which are then fed through the network to produce a single prediction:\n",
                "\n",
                "<img src=\"https://github.com/huggingface/workshops/blob/main/nlp-zurich/images/clf_arch.png?raw=1\" alt=\"Alt text that describes the graphic\" title=\"Title text\" width=600>\n",
                "\n",
                "## Pipeline  {.smaller}\n",
                "\n",
                "- We need to specify the task in the `pipeline()` function as follows;\n",
                "\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "sentiment_pipeline = pipeline('text-classification')"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "- When you run this code, you'll see a message about which Hub model is being used by default. \n",
                "\n",
                "- In this case, the `pipeline()` function loads the `distilbert-base-uncased-finetuned-sst-2-english` model, which is a small BERT variant trained on [SST-2](https://paperswithcode.com/sota/sentiment-analysis-on-sst-2-binary) which is a sentiment analysis dataset.\n",
                "\n",
                ":::{.callout-note}\n",
                "üí° The first time you execute the code, the model will be automatically downloaded from the Hub and cached for later use! \n",
                ":::\n",
                "\n",
                "## Run pipeline {.smaller}\n",
                "\n",
                "- Now we are ready to run our example through pipeline and look at some predictions:\n",
                "\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "sentiment_pipeline(text)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "- `Output`: [{'label': 'NEGATIVE', 'score': 0.9015464186668396}]\n",
                "\n",
                "- The model predicts negative sentiment with a high confidence which makes sense given that we have a disgruntled customer. \n",
                "\n",
                "- You can also see that the pipeline returns a list of Python dictionaries with the predictions. \n",
                "\n",
                "- We can also pass several texts at the same time in which case we would get several dicts in the list for each text one.\n",
                "\n",
                "# Named entity recognition (NER)\n",
                "\n",
                "## Basics\n",
                "\n",
                "- Instead of just finding the overall sentiment, let's see if we can extract **entities** such as organizations, locations, or individuals from the text. \n",
                "\n",
                "- This task is called named entity recognition, or NER for short. \n",
                "\n",
                "## Predict class for echa token\n",
                "\n",
                "- Instead of predicting just a class for the whole text **a class is predicted for each token**, as shown in the example below:\n",
                "\n",
                "\n",
                "\n",
                "<img src=\"https://github.com/huggingface/workshops/blob/main/nlp-zurich/images/ner_arch.png?raw=1\" alt=\"Alt text that describes the graphic\" title=\"Title text\" width=600>\n",
                "\n",
                "## Pipeline\n",
                "\n",
                "- We just load a pipeline for NER without specifying a model. \n",
                "\n",
                "- This will load a default BERT model that has been trained on the [CoNLL-2003](https://huggingface.co/datasets/conll2003) dataset:\n",
                "\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "ner_pipeline = pipeline('ner')"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Merge entities {.smaller}\n",
                "\n",
                "- When we pass our text through the model, we now get a long list of Python dictionaries, where each dictionary corresponds to one detected entity. \n",
                "\n",
                "- Since multiple tokens can correspond to a a single entity, we can apply an aggregation strategy that merges entities if the same class appears in consequtive tokens:\n",
                "\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "entities = ner_pipeline(text, aggregation_strategy=\"simple\")\n",
                "print(entities)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "- `Output`: [{'entity_group': 'ORG', 'score': 0.87900954, 'word': 'Amazon', 'start': 5, 'end': 11}, {'entity_group': 'MISC', 'score': 0.9908588, 'word': 'Optimus Prime', 'start': 36, 'end': 49}, {'entity_group': 'LOC', 'score': 0.9997547, 'word': 'Germany', 'start': 90, 'end': 97}, {'entity_group': 'MISC', 'score': 0.55656713, 'word': 'Mega', 'start': 208, 'end': 212}, {'entity_group': 'PER', 'score': 0.5902563, 'word': '##tron', 'start': 212, 'end': 216}, {'entity_group': 'ORG', 'score': 0.6696913, 'word': 'Decept', 'start': 253, 'end': 259}, {'entity_group': 'MISC', 'score': 0.4983487, 'word': '##icons', 'start': 259, 'end': 264}, {'entity_group': 'MISC', 'score': 0.77536064, 'word': 'Megatron', 'start': 350, 'end': 358}, {'entity_group': 'MISC', 'score': 0.987854, 'word': 'Optimus Prime', 'start': 367, 'end': 380}, {'entity_group': 'PER', 'score': 0.81209683, 'word': 'Bumblebee', 'start': 502, 'end': 511}]\n",
                "\n",
                "## Clean the output\n",
                "\n",
                "- This isn't very easy to read, so let's clean up the outputs a bit:\n",
                "\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "for entity in entities:\n",
                "    print(f\"{entity['word']}: {entity['entity_group']} ({entity['score']:.2f})\")"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "```json\n",
                "Amazon: ORG (0.88)  \n",
                "Optimus Prime: MISC (0.99)  \n",
                "Germany: LOC (1.00)  \n",
                "Mega: MISC (0.56)  \n",
                "##tron: PER (0.59)  \n",
                "Decept: ORG (0.67)  \n",
                "##icons: MISC (0.50)  \n",
                "Megatron: MISC (0.78)  \n",
                "Optimus Prime: MISC (0.99)  \n",
                "Bumblebee: PER (0.81)  \n",
                "```\n",
                "\n",
                "## Findings\n",
                "\n",
                "- It seems that the model found most of the named entities but was confused about \"Megatron\" andn \"Decepticons\", which are characters in the transformers franchise. \n",
                "\n",
                "- This is no surprise since the original dataset probably did not contain many transformer characters. For this reason it makes sense to further fine-tune a model on your on dataset!\n",
                "\n",
                "\n",
                "# Question answering\n",
                "\n",
                "## Basics {.smaller}\n",
                "\n",
                "- In this task, the model is given a **question** and a **context** and needs to find the answer to the question within the context. \n",
                "\n",
                "- This problem can be rephrased as a classification problem: For each token the model needs to predict whether it is the start or the end of the answer. \n",
                "\n",
                "## Basics\n",
                "\n",
                "- In the end we can extract the answer by looking at the span between the token with the highest start probability and highest end probability:\n",
                "\n",
                "\n",
                "\n",
                "<img src=\"https://github.com/huggingface/workshops/blob/main/nlp-zurich/images/qa_arch.png?raw=1\" alt=\"Alt text that describes the graphic\" title=\"Title text\" width=600>\n",
                "\n",
                "\n",
                "## Pipeline\n",
                "\n",
                "- we load the model by specifying the task in the `pipeline()` function:\n",
                "\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "qa_pipeline = pipeline(\"question-answering\")"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "- This default model is trained on the famous [SQuAD dataset](https://huggingface.co/datasets/squad).\n",
                "\n",
                "## Ask question\n",
                "\n",
                "- Let's see if we can ask it what the customer wants:\n",
                "\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "question = \"What does the customer want?\"\n",
                "\n",
                "outputs = qa_pipeline(question=question, context=text)\n",
                "outputs"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n",
                "```json\n",
                "{'score': 0.6312916874885559,\n",
                " 'start': 335,\n",
                " 'end': 358,\n",
                " 'answer': 'an exchange of Megatron'}\n",
                "```\n",
                "\n",
                "# Text summarization\n",
                "\n",
                "## Basics {background-color=\"white\"}\n",
                "\n",
                "- Generation is much more computationally demanding since we usually generate one token at a time and need to run this several times. \n",
                "\n",
                "- An example for how this process works is shown below:\n",
                "\n",
                "\n",
                "\n",
                "<img src=\"https://github.com/huggingface/workshops/blob/main/nlp-zurich/images/gen_steps.png?raw=1\" alt=\"Alt text that describes the graphic\" title=\"Title text\" width=600>\n",
                "\n",
                "## Pipeline\n",
                "\n",
                "- A popular task involving generation is summarization\n",
                "\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "summarization_pipeline = pipeline(\"summarization\")"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "- This model was trained on the [CNN/Dailymail dataset](https://huggingface.co/datasets/cnn_dailymail) to summarize news articles.\n",
                "\n",
                "## Output"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "outputs = summarization_pipeline(text, max_length=80, clean_up_tokenization_spaces=True)\n",
                "\n",
                "print(wrapper.fill(outputs[0]['summary_text']))"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "- `Output`:  Bumblebee ordered an Optimus Prime action figure from your online store in Germany. Unfortunately, when I opened the package, I discovered to my horror\n",
                "that I had been sent an action figure of Megatron instead. As a lifelong enemy\n",
                "of the Decepticons, I hope you can understand my dilemma.\n",
                "\n",
                "# Translation\n",
                "\n",
                "## Basics\n",
                "\n",
                "- But what if there is no model in the language of my data? \n",
                "\n",
                "- You can still try to translate the text. \n",
                "\n",
                "- The [Helsinki NLP team](https://huggingface.co/models?pipeline_tag=translation&sort=downloads&search=Helsinkie-NLP) has provided over 1,000 language pair models for translation. \n",
                "\n",
                "## Pipeline\n",
                "\n",
                "- Translate English to German:\n",
                "\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "translator = pipeline(\"translation_en_to_de\", model=\"Helsinki-NLP/opus-mt-en-de\")"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Output {.smaller}\n",
                "\n",
                "- Let's translate our text to German:\n",
                "\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "outputs = translator(text, clean_up_tokenization_spaces=True, min_length=100)\n",
                "\n",
                "print(wrapper.fill(outputs[0]['translation_text']))"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "- `Output`: Sehr geehrter Amazon, letzte Woche habe ich eine Optimus Prime Action Figur aus Ihrem Online-Shop in Deutschland bestellt. Leider, als ich das Paket √∂ffnete, entdeckte ich zu meinem Entsetzen, dass ich stattdessen eine Action Figur von Megatron geschickt worden war! Als lebenslanger Feind der Decepticons, Ich hoffe, Sie k√∂nnen mein Dilemma verstehen. Um das Problem zu l√∂sen, Ich fordere einen Austausch von Megatron f√ºr die Optimus Prime Figur habe ich bestellt. Eingeschlossen sind Kopien meiner Aufzeichnungen √ºber diesen Kauf. Ich erwarte,\n",
                "von Ihnen bald zu h√∂ren. Aufrichtig, Bumblebee.\n",
                "\n",
                "## Findings\n",
                "\n",
                "- We can see that the text is clearly not perfectly translated, but the core meaning stays the same. \n",
                "\n",
                "- Another  application of translation models is data augmentation via backtranslation\n",
                "\n",
                "# Zero-shot classification\n",
                "\n",
                "\n",
                "## Basics\n",
                "\n",
                "- In zero-shot classification the model receives a text and a list of candidate labels and determines which labels are compatible with the text. \n",
                "\n",
                "- Instead of having fixed classes this allows for flexible classification without any labelled data! \n",
                "\n",
                "- Usually this is a good first baseline!\n",
                "\n",
                "\n",
                "## Pipeline"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "zero_shot_classifier = pipeline(\"zero-shot-classification\",\n",
                "                                model=\"vicgalle/xlm-roberta-large-xnli-anli\")"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Text input\n",
                "\n",
                "Let's have a look at an example:"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "text = 'Dieses Tutorial ist gro√üartig! Ich hoffe, dass jemand von Hugging Face meine Hochschule besuchen wird :)'\n",
                "\n",
                "classes = ['Treffen', 'Arbeit', 'Digital', 'Reisen']"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Pipeline"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "zero_shot_classifier(text, classes, multi_label=True)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "```json\n",
                "{'sequence': 'Dieses Tutorial ist gro√üartig! Ich hoffe, dass jemand von Hugging Face meine Hochschule besuchen wird :)',\n",
                " 'labels': ['Digital', 'Arbeit', 'Treffen', 'Reisen'],\n",
                " 'scores': [0.7426563501358032,\n",
                "  0.6590237021446228,\n",
                "  0.517701268196106,\n",
                "  0.011237525381147861]}\n",
                "```\n",
                "\n",
                "- For longer and more domain specific examples this approach might suffer.\n",
                "\n",
                "# Going beyond text\n",
                "\n",
                "Transformers can also be used for domains other than NLP! \n",
                "\n",
                "## Basics {.smaller}\n",
                "\n",
                "There are many more pipelines that you can experiment with"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "from transformers import pipelines\n",
                "for task in pipelines.SUPPORTED_TASKS:\n",
                "    print(task)"
            ],
            "execution_count": null,
            "outputs": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "language": "python",
            "display_name": "Python 3 (ipykernel)"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}