{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Build and Host Machine Learning Demos with Gradio & Hugging Face\n",
                "\n",
                "\n",
                "## Why Demos?\n",
                "\n",
                "![](../images/gradio-website.png)\n",
                "\n",
                "\n",
                "- Developers can easily **present** their work to a wide audience\n",
                "\n",
                "- Increase **reproducibility** of machine learning research\n",
                "\n",
                "- Easily **identify and debug** failure points of models\n",
                "\n",
                "\n",
                "## Learning goals\n",
                "\n",
                "\n",
                "Learn how to:\n",
                "\n",
                "1. Build a quick demo for your machine learning model in Python using the `gradio` library\n",
                "2. Host the demos for free with Hugging Face Spaces\n",
                "3. Add your demo to the Hugging Face org for your class\n",
                "\n",
                "\n",
                "\n",
                "## Keras Org on Hugging Face\n",
                "\n",
                "- As a quick example of what we would like to build, check out the [Keras Org on Hugging Face](https://huggingface.co/keras-io)\n",
                "\n",
                "\n",
                "- Open any **Space** in your browser to use the model immediately\n",
                "\n",
                "\n",
                "# Gradio Basics\n",
                "\n",
                "`gradio` is a Python library that lets you build web demos simply by specifying the list of input and output **components** expected by your machine learning model. \n",
                "\n",
                "## Gradio components {.smaller}\n",
                "\n",
                "- Gradio comes with a bunch of predefined components for different kinds of machine learning models. \n",
                "\n",
                "- For an **image classifier**, the expected input type is an `Image` and the output type is a `Label`.\n",
                "\n",
                "- For a **speech recognition model**, the expected input component is an `Microphone` (which lets users record from the browser) or `Audio` (which lets users drag-and-drop audio files), while the output type is `Text`. \n",
                "\n",
                "- For a **question answering model**, we expect **2 inputs**: [`Text`, `Text`], one textbox for the paragraph and one for the question, and the output type is a single `Text` corresponding to the answer. \n",
                "\n",
                "- For all of the supported components, [see the docs](https://gradio.app/docs/)\n",
                "\n",
                "## Gradio prediction function\n",
                "\n",
                "- In addition to the input and output types, Gradio expects a third parameter, which is the prediction function itself. \n",
                "\n",
                "- This parameter can be ***any* regular Python function** that takes in parameter(s) corresponding to the input component(s) and returns value(s) corresponding to the output component(s)\n",
                "\n",
                "# Python setup"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "from IPython.display import IFrame\n",
                "import numpy as np\n",
                "import gradio as gr"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Image App\n",
                "\n",
                "## Custom Python function\n",
                "\n",
                "- Let's create a simple Python function\n",
                "\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "def sepia(image):\n",
                "    sepia_filter = np.array(\n",
                "        [[0.393, 0.769, 0.189],\n",
                "         [0.349, 0.686, 0.168],\n",
                "         [0.272, 0.534, 0.131]]\n",
                "    )\n",
                "    sepia_img = image.dot(sepia_filter.T)\n",
                "    sepia_img /= sepia_img.max()\n",
                "    return sepia_img\n"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Gradio Image App "
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "gr.Interface(  # <1>\n",
                "    fn=sepia,  # <2>\n",
                "    inputs=\"image\",  # <3>\n",
                "    outputs=\"image\"  # <4>\n",
                ").launch()  # <5>"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "1. We define an Gradio `Interface` using:\n",
                "2. a function `fn`\n",
                "3. Input component: `inputs` image\n",
                "4. Output component: `outputs` image\n",
                "5. ... and `launch` the app\n",
                "\n",
                "## Gradio Image App Interface\n",
                "\n",
                "![](../images/gradio_image_app.png)\n",
                "\n",
                "\n",
                "# Gradio Sound App \n",
                "\n",
                "Create an app that generates a musical tone when provided a few different parameters.\n",
                "\n",
                "## Gradio Sound App {.smaller}"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "def generate_tone(note, octave, duration):\n",
                "    sampling_rate = 48000\n",
                "    a4_freq, tones_from_a4 = 440, 12 * (octave - 4) + (note - 9)\n",
                "    frequency = a4_freq * 2 ** (tones_from_a4 / 12)\n",
                "    audio = np.linspace(0, int(duration), int(duration) * sampling_rate)\n",
                "    audio = (20000 * np.sin(audio * (2 * np.pi * frequency))).astype(np.int16)\n",
                "    return sampling_rate, audio\n",
                "\n",
                "\n",
                "gr.Interface(\n",
                "    generate_tone,  # function\n",
                "    [\n",
                "        gr.Dropdown([\"C\", \"C#\", \"D\", \"D#\", \"E\", \"F\", \"F#\",\n",
                "                    \"G\", \"G#\", \"A\", \"A#\", \"B\"], type=\"index\"),\n",
                "        gr.Slider(4, 6, step=1),\n",
                "        gr.Number(value=1, label=\"Duration in seconds\"),\n",
                "    ],\n",
                "    \"audio\",\n",
                "    title=\"Generate a Musical Tone!\"\n",
                ").launch()"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Gradio Sound App Interface\n",
                "\n",
                "![](../images/gradio_music_app.png)\n",
                "\n",
                "# Text-to-Speech App\n",
                "\n",
                "- It is especially easy to demo a `transformers` model from Hugging Face's Model Hub, using the special `gr.Interface.load` method. \n",
                "\n",
                "- Text-to-speech model built by Facebook:\n",
                "\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "gr.Interface.load(\"huggingface/facebook/fastspeech2-en-ljspeech\").launch()"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Gradio Text to Speech App Interface\n",
                "\n",
                "![](../images/gradio_txt_sp.png)\n",
                "\n",
                "# GPT-J App\n",
                "\n",
                "- Demo for [GPT-J](https://huggingface.co/EleutherAI/gpt-j-6B), a large language model & add a couple of examples inputs:\n",
                "\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "examples = [[\"A student at HdM Stuttgart is\"],\n",
                "            [\"In Stuttgart you can \"]]\n",
                "\n",
                "gr.Interface.load(\"huggingface/EleutherAI/gpt-j-6B\",\n",
                "                  examples=examples).launch()"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Gradio GP-J App Interface\n",
                "\n",
                "![](../images/gradio_gp_j.png)\n",
                "\n",
                "# Build your own App\n",
                "\n",
                "- You can go to the [Hugging Face Model Hub](https://huggingface.co/models) and pick any model that performs one of the tasks supported in the `transformers` library\n",
                "\n",
                "- Create a Gradio demo for that model using `gr.Interface.load`.\n",
                "\n",
                "# Host the Demo\n",
                "\n",
                "Host the Demo (for free) on Hugging Face Spaces\n",
                "\n",
                "## Workflow, 1 {.smaller}\n",
                "\n",
                "- Here are the steps to that (also see GIF in the next slide):\n",
                "\n",
                "1. First, create a Hugging Face account if you do not already have one, by visiting <https://huggingface.co/> and clicking \"Sign Up\"\n",
                "\n",
                "2. Once you are logged in, click on your profile picture and then click on \"New Space\" underneath it to get to this page: <https://huggingface.co/new-space>\n",
                "\n",
                "\n",
                "3. Give your Space a name and a license. Select \"Gradio\" as the Space SDK, and then choose \"Public\" if you are fine with everyone accessing your Space and the underlying code\n",
                "\n",
                "4. Then you will find a page that provides you instructions on how to upload your files into the Git repository for that Space. You may also need to add a `requirements.txt` file to specify any Python package dependencies.\n",
                "\n",
                "5. Once you have pushed your files, that's it! Spaces will automatically build your Gradio demo allowing you to share it with anyone, anywhere!\n",
                "\n",
                "## Workflow in a GIF\n",
                "\n",
                "- You may also choose \"public\" instead of \"private\" when you create your space\n",
                "\n",
                "![](https://huggingface.co/blog/assets/28_gradio-spaces/spaces-demo-finalized.gif)\n",
                "\n",
                "\n",
                "## Share your demo\n",
                "\n",
                "- You can embed your Gradio demo on any website -- in a blog, a portfolio page, or even in a colab notebook, \n",
                "\n",
                "- Example: Pictionary sketch recognition model below:\n",
                "\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "IFrame(src='https://hf.space/gradioiframe/abidlabs/Draw/+', width=1000, height=800)"
            ],
            "execution_count": null,
            "outputs": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "language": "python",
            "display_name": "Python 3 (ipykernel)"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}